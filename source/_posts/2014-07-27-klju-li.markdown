---
layout: post
title: "KL距離 "
date: 2014-07-27 23:08:27 +0800
comments: true
categories: ["NLP"]
---

<!-- more -->

在看`統計自然語言基礎`這本書看到的，又稱為相對熵，裡面有提到一種語言熵越大
代表能傳輸的訊息量越大，也會讓語言處理起來更困難，因為熵越大代表不確定性越大
更難預測。


下面幾個關鍵字要找時間重新看看，之前在大學上通訊原理跟信號處理有提到香農定理跟信噪比
之類的東西，當時是用來驗證某個編碼是否可以用來作為訊息傳輸用，沒有想到會在自然語言處理
的部分在遇到。

1. 信息論
2. 噪音信道模型
3. 熵
4. 混亂度