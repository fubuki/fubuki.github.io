<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Categories: NLP | Learning Blog]]></title>
  <link href="http://fubuki.github.io/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://fubuki.github.io/"/>
  <updated>2014-07-28T23:57:54+08:00</updated>
  <id>http://fubuki.github.io/</id>
  <author>
    <name><![CDATA[Fubuki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[KL距離]]></title>
    <link href="http://fubuki.github.io/blog/2014/07/27/klju-li/"/>
    <updated>2014-07-27T23:08:27+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/07/27/klju-li</id>
    <content type="html"><![CDATA[<!-- more -->


<p>在看<code>統計自然語言基礎</code>這本書看到的，又稱為相對熵，裡面有提到一種語言熵越大
代表能傳輸的訊息量越大，也會讓語言處理起來更困難，因為熵越大代表不確定性越大
更難預測。</p>

<p>下面幾個關鍵字要找時間重新看看，之前在大學上通訊原理跟信號處理有提到香農定理跟信噪比
之類的東西，當時是用來驗證某個編碼是否可以用來作為訊息傳輸用，沒有想到會在自然語言處理
的部分在遇到。</p>

<ol>
<li>信息論</li>
<li>噪音信道模型</li>
<li>熵</li>
<li>混亂度</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Discourse Analysis]]></title>
    <link href="http://fubuki.github.io/blog/2014/07/15/discourse-analysis/"/>
    <updated>2014-07-15T23:21:05+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/07/15/discourse-analysis</id>
    <content type="html"><![CDATA[<p>Discourse analysis : 在處理對話系統所建立的模型，用來分析句子跟句子之間的關係。<br/>
可參考<a href="http://en.wikipedia.org/wiki/Discourse_analysis">wiki</a>獲得更詳細的解釋。</p>

<h3>book</h3>

<ol>
<li>The Handbook of Discourse Analysis</li>
<li>How to do Discourse Analysis: A Toolkit</li>
</ol>

]]></content>
  </entry>
  
</feed>
