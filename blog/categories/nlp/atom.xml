<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Categories: NLP | Learning Blog]]></title>
  <link href="http://fubuki.github.io/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://fubuki.github.io/"/>
  <updated>2014-07-30T00:05:11+08:00</updated>
  <id>http://fubuki.github.io/</id>
  <author>
    <name><![CDATA[Fubuki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[語義消歧]]></title>
    <link href="http://fubuki.github.io/blog/2014/07/28/yu-yi-xiao-qi/"/>
    <updated>2014-07-28T23:32:04+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/07/28/yu-yi-xiao-qi</id>
    <content type="html"><![CDATA[<!-- more -->


<ol>
<li>WSD （Word Sense Disambiguation）</li>
<li>基於語料庫的消歧 (監督學習)</li>
<li>基於詞典的消歧</li>
<li>無監督消歧</li>
<li>POS  (Part-of-speech tagging)</li>
</ol>


<p>語義代表一個詞所擁有的不同的意義，一個詞會因為上下文的關係而有不同的意義，
也因此在做翻譯的時候會有混淆，不過目前我知道用來實作機器翻譯是用大量的統計
資料譬如 google的 Ngram 所形成的統計模型。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KL距離]]></title>
    <link href="http://fubuki.github.io/blog/2014/07/27/klju-li/"/>
    <updated>2014-07-27T23:08:27+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/07/27/klju-li</id>
    <content type="html"><![CDATA[<!-- more -->


<p>在看<code>統計自然語言基礎</code>這本書看到的，又稱為相對熵，裡面有提到一種語言熵越大
代表能傳輸的訊息量越大，也會讓語言處理起來更困難，因為熵越大代表不確定性越大
更難預測。</p>

<p>下面幾個關鍵字要找時間重新看看，之前在大學上通訊原理跟信號處理有提到香農定理跟信噪比
之類的東西，當時是用來驗證某個編碼是否可以用來作為訊息傳輸用，沒有想到會在自然語言處理
的部分在遇到。</p>

<ol>
<li>信息論</li>
<li>噪音信道模型</li>
<li>熵</li>
<li>混亂度</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Discourse Analysis]]></title>
    <link href="http://fubuki.github.io/blog/2014/07/15/discourse-analysis/"/>
    <updated>2014-07-15T23:21:05+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/07/15/discourse-analysis</id>
    <content type="html"><![CDATA[<p>Discourse analysis : 在處理對話系統所建立的模型，用來分析句子跟句子之間的關係。<br/>
可參考<a href="http://en.wikipedia.org/wiki/Discourse_analysis">wiki</a>獲得更詳細的解釋。</p>

<h3>book</h3>

<ol>
<li>The Handbook of Discourse Analysis</li>
<li>How to do Discourse Analysis: A Toolkit</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[中文分词的最大匹配法]]></title>
    <link href="http://fubuki.github.io/blog/2014/04/26/zhong-wen-fen-ci-de-zui-da-pi-pei-fa/"/>
    <updated>2014-04-26T22:49:17+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/04/26/zhong-wen-fen-ci-de-zui-da-pi-pei-fa</id>
    <content type="html"><![CDATA[<p>記錄一下中文分詞的入門演算法。<br/>
1. 最大正向匹配
2. 最大逆向匹配
3. 雙向匹配</p>

<!-- more -->


<p>中文分詞一開始是使用詞典作分詞，在分詞的過程中主要有幾個原則:  <br/>
1. 切出來的詞彙越長越好，越長就能代表越複雜的意思，句子的意思也能更明確。<br/>
2. 切出來的分詞結果通常會有一些停用詞(stop words)或是詞典裡沒有的詞導致會有多餘的字出現，那類的字越少越好。</p>

<p>然後最上面提到的三個演算法其實都是對詞典作匹配，只是是從句子彺前往後匹配或者兩種方法都使用，之後有空用程式語言實踐一下演算法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LSI 和 SVD]]></title>
    <link href="http://fubuki.github.io/blog/2014/04/19/lsi-he-svd/"/>
    <updated>2014-04-19T19:44:28+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/04/19/lsi-he-svd</id>
    <content type="html"><![CDATA[<p>記錄在處理nlp問題時看到的專有名詞。</p>

<!-- more -->


<p><a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent semantic indexing</a> (LSI) 照字面翻成潛在語義索引，一般搜尋引擎透過關鍵字索引文件，但是LSI是利用詞彙在不同語境下有不同語義去搜尋，
因為使用者在搜尋文件時有可能因為一詞多義讓搜尋結果夾雜多餘的文件，一義多詞則會讓一些文件沒有搜尋到的問題。</p>

<p><a href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent semantic analysis</a> (LSA) 潛在語義學，一般語義學主要研究詞彙的同義、相似、反義詞，而LSA主要是研究詞彙在文件中的關係，
LSI假設詞彙在類似的文句中會有相近的意思，LSI利用tf-idf建立一個大型矩陣，行為詞列為文件值為tf-idf算出的權值。</p>

<p><a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition</a> (SVD) 奇異值分解，一種矩陣分解方法，LSA使用SVD讓原本很大的矩陣降維。</p>

<p><a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> 找出一個詞彙在一個文件中的重要程度。</p>
]]></content>
  </entry>
  
</feed>
