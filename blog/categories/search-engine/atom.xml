<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Categories: Search Engine | Learning Blog]]></title>
  <link href="http://fubuki.github.io/blog/categories/search-engine/atom.xml" rel="self"/>
  <link href="http://fubuki.github.io/"/>
  <updated>2015-01-06T23:44:26+08:00</updated>
  <id>http://fubuki.github.io/</id>
  <author>
    <name><![CDATA[Fubuki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kibana 資料可視化項目]]></title>
    <link href="http://fubuki.github.io/blog/2014/12/24/kibana/"/>
    <updated>2014-12-24T22:57:33+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/12/24/kibana</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/elasticsearch/kibana">kibana</a> 是基於 ElasticSearch 建立的資料可視化項目</p>

<!-- more -->


<p>之前有使用 ElasticSearch 建立資料庫的搜尋服務，其中也有透過 logstash 收集伺服器上面的 LOG 檔案傳給 ElasticSearch
進行處理然後透過 API 呼叫取得處理過的資料顯示在後台上。</p>

<p>LOG檔的訊息通常大量並且雜亂的，而且都帶有時間標記，所以為了讓人使用這些大量的資料就必須要有個可以分析並且顯示的後台，
所以看到網路上有出一個 <a href="https://github.com/elasticsearch/kibana">kibana</a> 的資料可視化項目，</p>

<p><a href="https://github.com/elasticsearch/kibana">kibana</a> 安裝很簡單，比較麻煩的反而是將 LOG 檔透過 logstash 丟到 ElasticSearch 中處理，之後的步驟可以參考這個 <a href="http://kibana.logstash.es/">教程</a>
，建立所需要的環境之後便可以開始分析機器上面的 LOG 檔案。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Carrot2 : Clustering Engine]]></title>
    <link href="http://fubuki.github.io/blog/2014/05/12/carrot2-clustering-engine/"/>
    <updated>2014-05-12T22:22:53+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/05/12/carrot2-clustering-engine</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/carrot2/carrot2">Carrot2</a> 一個開源的分類引擎，之前在學習solr的時候就有看到這個開源專案，在<a href="http://search.carrot2.org/stable/search">search</a>有一個Carrot2的範例，
是針對搜尋引擎得到的結果做分類，Carrot2本身也可以和Solr集成，也能支援中文分詞器，之後研究一下內部使用哪些演算法和
如何跟solr結合。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SenseiDB]]></title>
    <link href="http://fubuki.github.io/blog/2014/03/23/senseidb/"/>
    <updated>2014-03-23T21:46:27+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/03/23/senseidb</id>
    <content type="html"><![CDATA[<p><a href="http://senseidb.com">sensei</a>是以前在學習hadoop時找到的，是一款跟hadoop集成的搜尋系統，第一次看到是在兩年前，最近在VM發現當時的建立的痕跡，
特此紀錄一下以便日後研究。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch]]></title>
    <link href="http://fubuki.github.io/blog/2014/03/03/elasticsearch/"/>
    <updated>2014-03-03T21:51:01+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/03/03/elasticsearch</id>
    <content type="html"><![CDATA[<p>elasticsearch 另外一款搜尋引擎，之前就有在注意了，目前已經有出現1.0版本，<br/>
不過底層跟solr都是lucene，使用上要去抓取資料庫的內容要另外設定，我比較中意
Solr只要將欄位設定好就可以直接使用。</p>

<!-- more -->


<p>目前維基百科和github都用elasticsearch解決他們搜尋功能，不知道背後是如何運用的，<br/>
這邊要使用了話主要問題還是在cjk分詞，之前是用詞典的方式，如果換用HMM或是n-gram的方式
來增加分詞的精確度是最要緊的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solr]]></title>
    <link href="http://fubuki.github.io/blog/2014/03/01/solr/"/>
    <updated>2014-03-01T09:50:57+08:00</updated>
    <id>http://fubuki.github.io/blog/2014/03/01/solr</id>
    <content type="html"><![CDATA[<h2>Apache Solr</h2>

<p>目前用我用最多的開源搜尋引擎，還蠻好用的，主要用在database的全文搜尋，中文分詞的部分是配合IKAnalyzer分词<br/>
Server部分使用solr自帶jetty或是使用tomcat。</p>
]]></content>
  </entry>
  
</feed>
