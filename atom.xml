<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Learning Blog]]></title>
  <link href="http://fubuki.github.io/atom.xml" rel="self"/>
  <link href="http://fubuki.github.io/"/>
  <updated>2015-08-17T23:35:56+08:00</updated>
  <id>http://fubuki.github.io/</id>
  <author>
    <name><![CDATA[Fubuki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OTR vs PGP 加密]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/17/otr-vs-pgp/"/>
    <updated>2015-08-17T23:21:01+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/17/otr-vs-pgp</id>
    <content type="html"><![CDATA[<!-- more -->


<p><code>Off-the-Record Communication, or, Why Not To Use PGP</code> 兩種加密協議的比較，之前在聽關於使用
xmpp 協定跟別人聊天的 workshop，裡面提到了 OTR 跟 PGP 這兩種加密方式，不過經過研究 PGP 加密有些問題，
比較推薦使用 OTR，。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coscup 2015 Day2]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/16/coscup-2015-day2/"/>
    <updated>2015-08-16T20:02:47+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/16/coscup-2015-day2</id>
    <content type="html"><![CDATA[<!-- more -->


<h3>How Redis Powers Your Web Service &amp; Flurry analytics Intro</h3>

<p>Yahoo APAC Data Team</p>

<p>Yahoo Redis 是怎麼使用的</p>

<ol>
<li>Sentinel</li>
<li>twemproxy</li>
<li>redis cluster</li>
</ol>


<p>幾乎以前就聽過了</p>

<h4>Flurry Analytics (Yahoo mobile developer suite)</h4>

<p>手機資料分析工具</p>

<h3>Personalized Search in Yahoo Taiwan eCommerce Sites</h3>

<ol>
<li>搜尋經驗優化</li>
<li>個人化搜尋</li>
<li>演算法和資料量</li>
<li>使用哪些開源工具</li>
</ol>


<p>重要是 machine learning ranking 應該就是</p>

<p>機器學習用哪個算法
資料集怎麼來的</p>

<p>商品了解</p>

<p>重要產品標籤辨識
LDA SVM</p>

<p>重要產品</p>

<p>使用者偏好推定</p>

<p>巨量資料跟即時運算的問題</p>

<p>PIG 的用法</p>

<p>PLSI EM algorithm</p>

<p>mahout => spark MLlib</p>

<p>K-means</p>

<p>spark 可以加速運算</p>

<p>Logistic Regression</p>

<p>decision tree
使用 Random forest  加快</p>

<p>使用者短期行為處理</p>

<p>Storm</p>

<h3>Neural Turing Machines Implementation</h3>

<p>HMM => Speech recognition</p>

<p>SVM => Image recogniton</p>

<p>CNN</p>

<p>RNN</p>

<p>=> Neural Turing Machine</p>

<p>新的論文可以看看</p>

<h3>堆疊之下</h3>

<p>有做 information retrival</p>

<ol>
<li><p>Data-Driven 資料導向</p>

<ul>
<li>A/B testing</li>
<li>MAB testing</li>
</ul>
</li>
<li><p>Dev Ops 分工</p></li>
<li><p>IT部門 ~900 人,小隊伍 (6人)</p></li>
</ol>


<p>主要講 Booking 的背後使用哪些語言跟軟體</p>

<h3>自己的機器人自己做</h3>

<p>使用 arduino 製作機器人</p>

<p>目前尚未完成</p>

<h3>從UAV到UAS 開源無人機的技術與下一步</h3>

<p>AHRS 系統跟多軸飛行器使用的組件跟用途</p>

<h3>「封麥」演說：台灣開放原始碼生態圈回顧</h3>

<h3>Lightning Talk</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coscup 2015 Day1]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/15/coscup-2015-day1/"/>
    <updated>2015-08-15T22:09:54+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/15/coscup-2015-day1</id>
    <content type="html"><![CDATA[<!-- more -->


<h3>BYOC: Build Your Own COSCUP</h3>

<p>介紹 coscup 從以前到現在的演進跟內部怎麼分工</p>

<h3>中文排版需求以及我在W3C看到的事情</h3>

<p>中文排版需求第一份使用中文撰寫文檔</p>

<p>講解 epub 演進的歷史
假裝一切都做好了是最麻煩的</p>

<p>中文排版規格挖坑</p>

<p>UA Stylesheet 導致中文排版問題的元兇</p>

<p>Round Display 給圓形螢幕使用的  CSS</p>

<p><code>light-level</code>  為 media query 的屬性可以根據光源調整 css</p>

<p>CSS SECRETS 一本書可以看看</p>

<p>Text-wrap :balance</p>

<p>CSS scrollbar 微軟提出的用來實做常見的捲動效果</p>

<h3>PIME - 用 &ldquo;Python&rdquo; 快速開發 Windows 的中文輸入法</h3>

<p>看到 PCMAN 本人</p>

<p>講解了 windows 上面輸入法的差別，基本上分成兩種在加上 32 位元跟 64 位元所以會有四種情形。</p>

<p>Demo 了喵喵輸入法</p>

<p>主要是使用了 Client-Server 的架構，所以只要更改 Server 的程式便可以快速修改輸入法的功能。</p>

<h3>Linux 桌面系統在繁體中文資訊化的發展回顧</h3>

<p>這場主要是講 Linux 跟一些應用軟體的中文化情形，太少人參與軟體中文化，應該說
以前的使用者會回饋一些 patch 回來，不過現在</p>

<p>CLE</p>

<p>CLDP</p>

<h3>workshop</h3>

<p>跑去參加 xmpp 的 workshop，這場收穫比較大的是修正一些以前對於 xmpp 的錯誤概念，
之後實做聊天軟體時可以參考看看。</p>

<h3>A Fresh Look at Accessibility (on the web and mobile devices)</h3>

<p>無障礙網路空間</p>

<p>講一些目前 w3c 的規範</p>

<h3>第一次自幹 Open Data SimCity 就上手</h3>

<p>google map + d3.js 實做資料視覺化</p>

<p>主要在 Demo 效果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Splainer : Explain Solr Result]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/14/splainer/"/>
    <updated>2015-08-14T22:52:43+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/14/splainer</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://github.com/o19s/splainer">splainer</a> 是可以利用 solr 的搜尋結果分析背後的分數，跟使用 Lucene 的 explain api 得出背後的得分類似。
。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[針對短文本做語言偵測]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/13/short-text-language-detect/"/>
    <updated>2015-08-13T21:08:49+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/13/short-text-language-detect</id>
    <content type="html"><![CDATA[<!-- more -->


<p>針對類似 <code>tweet</code> 的短文本進行語言偵測。以前有用過 Tika 實做語言偵測不過命中率從別人的測試數據看起來不太好，
之後從別的地方聽到了 <a href="https://github.com/CLD2Owners/cld2">cld2</a> 效果會比較好，不過在對很文本長度不長情形下準度會下降，所以另外研究了 <a href="http://www.slideshare.net/shuyo/short-text-language-detection-with-infinitygram-12949447">Short Text Language Detection with Infinity-Gram</a>
使用其他方法嘗試解決這類的問題。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Topic Model 取出文章的主題]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/12/topic-model/"/>
    <updated>2015-08-12T23:18:23+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/12/topic-model</id>
    <content type="html"><![CDATA[<!-- more -->


<p>之前有使用 Text Rank 取出文章的摘要時另外看到了使用 LDA 之類的方法處理類似需求，這類的方法被稱為
 <code>Topic Model</code> ，研究下去可以發現有很多種類的模型可以使用不過大致上基於 LDA 的變種，學好 LDA 後其他就
 能夠比較容易理解。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[InnoDB 一些相關概念]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/11/innodb-keyword/"/>
    <updated>2015-08-11T23:17:51+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/11/innodb-keyword</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="http://www.xaprb.com/blog/2015/08/08/innodb-book-outline/">An Outline for a Book on InnoDB</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 CRF 做中文分詞]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/10/conditional-random-fields/"/>
    <updated>2015-08-10T22:01:52+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/10/conditional-random-fields</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="http://www.inference.phy.cam.ac.uk/hmw26/crf/">CRF</a> 目前手上主要用來做新詞辨識的工作，在其他領域也不少用途，某方面跟 HMM 有些類似的地方。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TextRank 算法]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/09/textrank/"/>
    <updated>2015-08-09T22:50:36+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/09/textrank</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">TextRank: Bringing Order into Texts</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git 2.5 Release]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/08/git-worktrees/"/>
    <updated>2015-08-08T22:45:36+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/08/git-worktrees</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Git 釋出 2.5 版本，Github 有寫一篇文章作介紹 <a href="https://github.com/blog/2042-git-2-5-including-multiple-worktrees-and-triangular-workflows">Git 2.5, including multiple worktrees and triangular workflows</a>，
不過主要讓我感興趣的是新增了一個 <code>git worktree</code> 的指令 。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[建立詞庫和相似詞表]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/07/nlp4l/"/>
    <updated>2015-08-07T22:43:03+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/07/nlp4l</id>
    <content type="html"><![CDATA[<!-- more -->


<p>這幾天在研究怎麼自己建立詞庫和相似詞表，目前有看到有看一個可以不會太難實做的方法，主要是參考下面三篇文章，
雖然是基於日本語所做的處理不過可以也套用到中文上，裡面大部分是透過 Wikipedia 的資料做處理，我這邊應該會換用從
網路上爬來的新聞資料試做看看。</p>

<ol>
<li><a href="http://www.slideshare.net/KojiSekiguchi/wikipediasolr">WikipediaからのSolr用類義語辞書の自動生成</a></li>
<li><a href="http://www.slideshare.net/KojiSekiguchi/lucene-terms-extraction">Lucene terms extraction</a></li>
<li><a href="http://lucene.jugem.jp/?eid=482">日本語Wikipediaからの類義語辞書の自動生成</a></li>
</ol>


<p>另外上面的文章有提到一個使用 scala 撰寫的工具名叫 <a href="https://github.com/NLP4L/nlp4l">nlp4l</a> ，裡面似乎提供了不少自然語言處理的工具，如果透過這個工具
也許可以優化一些基於 Lucene 的搜尋引擎。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 Grafana 做為 Dashboard 後台]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/06/grafana/"/>
    <updated>2015-08-06T23:07:16+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/06/grafana</id>
    <content type="html"><![CDATA[<!-- more -->


<p>最近手上有不少 Log 檔所以想用來做一些分析， 以前有想過用 norikra 不過目前手上的環境是用沒辦法使用 jruby 所以想用其他替代
品，之前有思考 opentsdb 和 influxdb 這兩套，然後還缺一個 dashboard 後台，這邊有想過使用 Kibana 不過這個通常會搭配 elasticsearch
 使用，所以我換用 <a href="https://github.com/grafana/grafana">grafana</a> 使用 golang 寫的一個後台，可以搭配 opentsdb 和 influxdb，最後還缺要考慮擴展和即時計算的問題。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mitmproxy 安裝和使用]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/05/mitmproxy/"/>
    <updated>2015-08-05T22:52:01+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/05/mitmproxy</id>
    <content type="html"><![CDATA[<!-- more -->


<p>之前開發給手機的 API 都是直接看 <code>server log</code>，然而最近看到有個專案 <a href="https://github.com/mitmproxy/mitmproxy">mitmproxy</a> 可以使用看看，可以直接做為代理
記錄 client 的呼叫，之前有想在 apache 上裝套件方便監視所有 API 的呼叫和傳輸資料，不過現在有這個就不需要了，另外
也可以自行撰寫指令搞擴充想要的功能。</p>

<h3>安裝</h3>

<p>直接用 pipe 安裝就好</p>

<pre><code>pip install mitmproxy
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FST and FSA]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/04/finite-state-transducer/"/>
    <updated>2015-08-04T23:00:52+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/04/finite-state-transducer</id>
    <content type="html"><![CDATA[<!-- more -->


<p>在使用 Lucene 的時候看到在 4.0 增加了 FST 的方式處理自然語言，之前在自然語言處理綜論這本書上有看到關於 FST 的概念，
其實就是 FSM ，這邊有一個使用 FST 的專案 <a href="https://github.com/OpenSextant/SolrTextTagger">SolrTextTagger</a> 是在一份關於如何處理多語言環境的簡報上看到的，剛好有這些契機，
而且似乎可以用 FST 處理關於同義詞的問題，正好可以拿來研究。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lucene 索引格式]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/03/lucene-index-format/"/>
    <updated>2015-08-03T23:27:09+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/03/lucene-index-format</id>
    <content type="html"><![CDATA[<!-- more -->


<p>最近看了一些關於如何讓 <code>solr</code> 處理超大索引的研究，不過都沒有提到關於 lucene 索引格式的資料，
<a href="http://hackerlabs.org/blog/2011/10/01/hacking-lucene-the-index-format/">Hacking Lucene - The Index format</a> 裡面有些資料結構是以前看過的，例如 <code>skip list</code> 之類的，由於
之後有可能需要更改或是把一些 lucenc/solr 在 jira 上的 patch 更有可能需要更改索引內容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Skip List and Lucene Index]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/03/skip-list/"/>
    <updated>2015-08-03T22:31:55+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/03/skip-list</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://en.wikipedia.org/wiki/Skip_list">Skip list</a> 之前在看 redis 內部結構的時候遇到，然後最近在研究關於 solr index 優化問題的時後也提到
這個資料結構，用途上類似紅黑樹但是實現起來比較簡單。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solr 跨資料中心的問題]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/02/solr-cross-data-center-replication/"/>
    <updated>2015-08-02T23:06:13+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/02/solr-cross-data-center-replication</id>
    <content type="html"><![CDATA[<!-- more -->


<p>在看一篇 Apple 如何使用 solrcould 的演講上提到一些關於 <code>solr issue</code> ，然後其中有個是關於 solr 在跨資料中心的問題，
裡面有提到一些如果 Solr 佈署在不同地區的資料中心會遇到哪些問題值得研究 <a href="http://yonik.com/solr-cross-data-center-replication/">Solr Cross Data Center Replication</a> 。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[圖片搜尋的演算法]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/01/perceptual-hash-algorithm/"/>
    <updated>2015-08-01T23:42:19+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/01/perceptual-hash-algorithm</id>
    <content type="html"><![CDATA[<!-- more -->


<p>之前有在找 google 以及其他網站的以圖找圖服務是怎麼做的，以前想過應該是用 hash 的方式比對會比較快，
不過不知道是用怎麼樣的 hash 演算法，然後最近看到了 <a href="https://en.wikipedia.org/wiki/Perceptual_hashing">Perceptual hashing</a>，似乎可以用來做圖片搜尋的演算法，
不過不知道精準度如何，另外還有 <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a> 取出特徵點進行分析以及另外一種 <a href="http://www.phash.org/">pHash</a> 的算法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Information and Influence Propagation]]></title>
    <link href="http://fubuki.github.io/blog/2015/07/31/information-and-influence-propagation/"/>
    <updated>2015-07-31T23:41:31+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/07/31/information-and-influence-propagation</id>
    <content type="html"><![CDATA[<!-- more -->


<p>這個月看了一本關於社群網路分析的書，裡面一個蠻吸引我的是關於資訊傳播的模型，一個訊息出現後怎麼樣
發送以及感染也就是傳播給其他人，目前有本 <code>Information and Influence Propagation in Social Networks</code> 可以研究看看，
另外手上有本複雜網路的書也可以參考參考，不過還是希望找找看有沒有相關的開放式課程可以學習。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Skein Hash]]></title>
    <link href="http://fubuki.github.io/blog/2015/07/30/skein-hash/"/>
    <updated>2015-07-30T23:42:28+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/07/30/skein-hash</id>
    <content type="html"><![CDATA[<!-- more -->


<p>最近在研究怎麼可以快速取得大檔案的 hash function 剛好看到這個算法 <a href="https://en.wikipedia.org/wiki/Skein_%28hash_function%29">Skein (hash function)</a> 就記錄一下</p>
]]></content>
  </entry>
  
</feed>
