<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Learning Blog]]></title>
  <link href="http://fubuki.github.io/atom.xml" rel="self"/>
  <link href="http://fubuki.github.io/"/>
  <updated>2015-09-14T23:41:22+08:00</updated>
  <id>http://fubuki.github.io/</id>
  <author>
    <name><![CDATA[Fubuki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MLDM Monday --- 中文搜尋經驗分享]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/14/mldm-monday-zhong-wen-sou-xun-jing-yan-fen-xiang/"/>
    <updated>2015-09-14T23:26:59+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/14/mldm-monday-zhong-wen-sou-xun-jing-yan-fen-xiang</id>
    <content type="html"><![CDATA[<!-- more -->


<p>今天去參加一場 Meetup 內容是 <a href="http://blog.liang2.tw/2015Talk-Chinese-Search/#cover">中文搜尋經驗分享</a>，主要是講 Pinkoi 怎麼使用 elasticsearch 建立電商搜尋服務。</p>

<p>內容是先簡介一下 elasticearch 然後解說怎麼使用  jieba ，並且使用一些繁體中文資源建立 jieba 分詞用的詞典，
這邊是使用 wikipedia 和萌典，不過由於 jieba 是中國大陸那邊開發的所以有些訓練出來的模型像是 HMM 是針對簡體中文，
需要另外重新訓練或是處理。</p>

<p>最後就是介紹 word2vec，使用斷詞過的 wikipedia + gensim 建立資料，這是為了處理使用者搜尋不到的情形，找尋相關的關鍵字。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fish-shell]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/13/fish-shell/"/>
    <updated>2015-09-13T21:53:01+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/13/fish-shell</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://github.com/fish-shell/fish-shell">fish-shell</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec 建立本體模型]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/12/word2vec/"/>
    <updated>2015-09-12T23:41:44+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/12/word2vec</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://github.com/piskvorky/gensim/">gensim</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[顯示 Mecab 分詞的方式]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/11/n-best-search-algorithm/"/>
    <updated>2015-09-11T22:48:43+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/11/n-best-search-algorithm</id>
    <content type="html"><![CDATA[<!-- more -->


<ol>
<li><a href="http://d.hatena.ne.jp/a_bicky/20150413/1428872753">MeCab で N-Best 解の累積コストを出力する</a></li>
<li><a href="http://www.mwsoft.jp/programming/munou/mecab_nitteretou.html">日本テレビ東京で学ぶMeCabのコスト計算</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Count–min Sketch Algorithm]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/10/count-min-sketch-algorithm/"/>
    <updated>2015-09-10T23:40:34+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/10/count-min-sketch-algorithm</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch">Count–min sketch</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LTP: Language Technology Platform]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/09/ltp-language-technology-platform/"/>
    <updated>2015-09-09T23:05:41+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/09/ltp-language-technology-platform</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://github.com/HIT-SCIR/ltp">ltp</a> 哈爾濱工業大學(哈工大)提供的中文自然語言處理平台，之前有看到哈工大有一提供
一個線上的版本給人使用，而這個 <a href="https://github.com/HIT-SCIR/ltp">ltp</a> 應該就是提供給別人自行研究擴展的版本。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kagome 分詞器]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/08/kagome/"/>
    <updated>2015-09-08T23:36:54+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/08/kagome</id>
    <content type="html"><![CDATA[<!-- more -->


<ol>
<li><a href="https://github.com/ikawaha/kagome">kagome</a></li>
<li><a href="http://qiita.com/ikawaha/items/ff27ac03e22b7f36811b">Pure Go で辞書同梱な形態素解析器 kagome を公開してみました</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PHP 處理 Unicode 字元]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/07/php-regexp-unicode/"/>
    <updated>2015-09-07T23:06:35+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/07/php-regexp-unicode</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="http://php.net/manual/en/regexp.reference.unicode.php">Unicode character properties</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NPYLM 和 HPYLM]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/06/npylm/"/>
    <updated>2015-09-06T22:40:48+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/06/npylm</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://www.youtube.com/watch?v=GrTgJi65-Mg">理論はどうでもいいから作ってみたい人のためのNPYLM + 実装のための解説</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 Stream2es 匯入 Wikipedia 資料]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/05/elasticsearch-wikipedia-stream2es/"/>
    <updated>2015-09-05T23:38:48+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/05/elasticsearch-wikipedia-stream2es</id>
    <content type="html"><![CDATA[<!-- more -->


<p>目前手上已經有一些 Wikipedia 的資料，這是之前在使用 nlp4l 做為輸入資料遺留下來的，
那時使用了 json-wikipedia 將 xml 轉換成 json ，使用 jq 看過一遍大致上了解了內容的
資料結構，不過由於資料太過於龐大搜尋起來很久，所以使用了之前看到的工具 stream2es 將
手上的資料轉換到 elasticsearch 裡面。</p>

<p>最後轉換的結果有點少，只有一百九十萬多筆，有些資料似乎沒有匯入進去，之後要換用 Logstash
 匯入應該會有不一樣的結果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wikipedia Dump Data 解析]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/04/wikipedia-analysis/"/>
    <updated>2015-09-04T23:47:53+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/04/wikipedia-analysis</id>
    <content type="html"><![CDATA[<!-- more -->


<p>預定之後要處理 <a href="http://www.slideshare.net/ghazel7/wikipedia-17666978">Wikipedia解析</a> 的事情。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch 適合的 Shard Size]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/03/elasticsearch-shard-size/"/>
    <updated>2015-09-03T23:12:17+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/03/elasticsearch-shard-size</id>
    <content type="html"><![CDATA[<!-- more -->


<p>最近在思考 elasticsearch 優化的部分，當 index 不斷擴大的時候怎麼處理，目前是想到將資料做
 shard 然後使用多個 node 加快搜尋的速度，但是這邊有個問題是每個 shard 的大小要怎麼決定，
 這邊就暫時參考 <a href="http://engineering.datarank.com/2015/07/08/balancing-elasticsearch-cluster-by-shard-size.html">Balancing an Elasticsearch Cluster by Shard Size</a> 裡面提到的指標和工具測試看看。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Doc2Vec 相關]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/02/doc2vec-papper/"/>
    <updated>2015-09-02T23:28:43+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/02/doc2vec-papper</id>
    <content type="html"><![CDATA[<!-- more -->


<p>之前已經在 python 上玩過 word vectors ，而最近看到一篇關於 paragraph vector 的論文 <a href="http://arxiv.org/abs/1405.4053">Distributed Representations of Sentences and Documents</a>
需要了解一下差別和用途， <a href="https://github.com/mesnilgr/iclr15">iclr15</a> 則是另外一篇跟 <code>paragraph vector</code> 相關論文裡的程式碼，可以嘗試看看有沒有達成論文中的效果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mecab 使用 Wikipedia 增加詞庫]]></title>
    <link href="http://fubuki.github.io/blog/2015/09/01/mecab-wikipedia/"/>
    <updated>2015-09-01T23:36:16+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/09/01/mecab-wikipedia</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Mecab 可以自行增加詞庫的內容，增加的內容從目前看到的資料有幾個可以使用，主要是要轉換成
Mecab 可以吃的格式，</p>

<ol>
<li>Wikipedia jp</li>
<li>NicoNico</li>
<li>hatenaキーワード</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[語言處理的練習集]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/31/yan-yu-chu-li-100ben-falsetuku-2015/"/>
    <updated>2015-08-31T23:26:04+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/31/yan-yu-chu-li-100ben-falsetuku-2015</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/">言語処理100本ノック 2015</a> 還不錯的練習題目，裡面提到語言處理的基礎概念並且提供了一些練習題，
這些項目在做 NLP 都會遇到可以好好研究。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[建立自己的 Dotfiles]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/30/dotfile/"/>
    <updated>2015-08-30T22:30:11+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/30/dotfile</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://github.com/webpro/awesome-dotfiles">awesome-dotfiles</a></p>

<p><a href="http://qiita.com/b4b4r07/items/b70178e021bef12cd4a2">最強の dotfiles 駆動開発と GitHub で管理する運用方法</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[更換 Lucene Kuromoji 使用的 Dic]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/29/lucene-mecab-kuromoji/"/>
    <updated>2015-08-29T22:37:25+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/29/lucene-mecab-kuromoji</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Lucene 內部是使用 Kuromoji 對日本語做處理，而 Kuromoji 底層是使用 mecab 的  MeCab-IPADIC dictionary，
所以如果要加新詞其實可以透過 mecab 增加詞彙。</p>

<ol>
<li><a href="https://github.com/neologd/mecab-ipadic-neologd">mecab-ipadic-neologd</a></li>
<li><a href="http://d.hatena.ne.jp/Kazuhira/20150316/1426520209">修正されたmecab-ipadic-neologdの辞書を、Lucene Kuromojiに適用してみる</a></li>
<li><a href="http://qiita.com/wakisuke/items/d15b5defc1aad61cc910">mecabの辞書を自動コストで作成</a></li>
<li><a href="http://www.mwsoft.jp/programming/munou/mecab_dic_perform.html">IPA、NAIST、UniDic、JUMANの辞書実演比較</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Autocomplete 功能]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/28/elassticsearch-suggest/"/>
    <updated>2015-08-28T21:54:13+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/28/elassticsearch-suggest</id>
    <content type="html"><![CDATA[<!-- more -->


<p>elasticsearch 的 suggest 主要分成下面三種</p>

<ol>
<li>Phrase Suggester</li>
<li>Term suggester</li>
<li>Completion Suggester</li>
</ol>


<p>其中 <code>Completion Suggester</code> 目前看起來最適合用來實做 autocomplete 功能，使用上可以搭配  <code>category</code> 可以
增強 autocomplete 的功能，<code>category</code> 能夠自行添加欄位讓開發者可以對資料進行過濾。</p>

<p>目前看起來有幾點要注意:</p>

<ol>
<li>Completion Suggester 可以透過加上 weight 調整排序。</li>
<li>Completion Suggester 有 input 和 output 的欄位，可以讓多個輸入對應到單一輸輸出。</li>
<li>如果有多個 document 相同 output 那 elasticsearch 會只輸出一個。</li>
<li>Category 可以參照 <a href="https://www.elastic.co/blog/elasticsearch-1-2-adding-context-suggestions">Elasticsearch 1.2: Adding Context to Suggestions</a> 裡面有個使用 Category 過濾的例子。</li>
</ol>


<h4>建立 index 和 mapping file</h4>

<h4>加入 document</h4>

<h4>更新 document</h4>

<h4>搜尋</h4>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala 和 Sbt 的教學]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/27/scala-sbt-jiao-xue/"/>
    <updated>2015-08-27T23:24:16+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/27/scala-sbt-jiao-xue</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://github.com/twitter/scala_school">scala school</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[500px 如何分析資料]]></title>
    <link href="http://fubuki.github.io/blog/2015/08/26/500px-data-analytics/"/>
    <updated>2015-08-26T23:39:02+08:00</updated>
    <id>http://fubuki.github.io/blog/2015/08/26/500px-data-analytics</id>
    <content type="html"><![CDATA[<!-- more -->


<p><a href="https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83">Building Analytics at 500px</a></p>
]]></content>
  </entry>
  
</feed>
